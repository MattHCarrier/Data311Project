---
title: "Final-Project-Analysis"
author: "Raphael Chevallier, Ty Kreway, Matt Carrier, Mark Meyer, Simran Tejay"
date: "4/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

REQUIRED PACKAGES
```{r}
library(tree)
library(gclus)
library(caret)
library(MLmetrics)
library(tidyr)
library(RColorBrewer)
library(scales)
library(rgl)
library(cluster)
library(mclust)
library(MASS)
library(class)
library(randomForest)
library(NeuralNetTools)
library(neuralnet)
```

Setting up environment with dataset
```{r}
#set whatever directory you have your data in
data = read.csv("Cost_of_living_index.csv", header = TRUE)
attach(data)
set.seed(450)
```

Basic Visualization of the data and getting acquainted with it
```{r}
dim(data) #get the dimensiality of dataset
sapply(data, class) #get the class of the dataset
levels(data$City) #see all the different factors in the City variable
summary(data) #get the basic summary of the whole dataset
```

Variable Selection with Linear Regression Analysis
```{r}
#we will perform a linear regression to predict rank based on all other variables
#We just have to get rid of the numerical predictor ()
CL=data.frame(data$Rank,data$Cost.of.Living.Index,data$Rent.Index,data$Cost.of.Living.Plus.Rent.Index,data$Groceries.Index,data$Restaurant.Price.Index,data$Local.Purchasing.Power.Index)
head(CL)

lm<- lm(Rank ~ ., data=CL)
summary(lm)
```

Preforming Backward Selection
```{r}

summary(lm(Rank ~ ., data=CL))
# The predictor with the greatest p-value is Local Purchasing Power Index (with a p-value of 0.956), so we will remove that one

summary(lm(Rank ~ CL$data.Cost.of.Living.Index+CL$data.Rent.Index+CL$data.Cost.of.Living.Plus.Rent.Index+CL$data.Groceries.Index+CL$data.Restaurant.Price.Index, data=CL))
# The remaining predictor with the greatest p-value is Cost of Living Index (with a p-value of 0.75946), so we will remove that one.
#This is interesting as one would assume cost of living would be highly related to the rank of the city.

summary(lm(Rank ~ CL$data.Rent.Index+CL$data.Cost.of.Living.Plus.Rent.Index+CL$data.Groceries.Index+CL$data.Restaurant.Price.Index, data=CL))
# The remaining predictor with the greatest p-value is Restaurant Price Index (with a p-value of 0.39982), so we will remove that one  

summary(lm(Rank ~ CL$data.Rent.Index+CL$data.Cost.of.Living.Plus.Rent.Index+CL$data.Groceries.Index, data=CL))
#all significant (p-values < 0.05)

finalLinearMod= (lm(Rank ~ CL$data.Rent.Index+CL$data.Cost.of.Living.Plus.Rent.Index+CL$data.Groceries.Index, data=CL))
``` 

Multiple Linear Regression Analysis
```{r}
set.seed(450)
fit <- lm(data$Rank~data$Cost.of.Living.Index+data$Rent.Index+data$Cost.of.Living.Plus.Rent.Index+data$Groceries.Index+data$Restaurant.Price.Index+data$Local.Purchasing.Power.Index)
summary(fit)
```

Regression Tree Analysis
```{r}
set.seed(450)
data1 <- data[,-2]
head(data1)
simtree <- tree(Rank~Cost.of.Living.Index+Rent.Index+Cost.of.Living.Plus.Rent.Index+Groceries.Index+Restaurant.Price.Index+Local.Purchasing.Power.Index,data = data1)
summary(simtree)
plot(simtree)
text(simtree, pretty=0)
```

Regression Tree Analysis with "Cost of Living Index" removed.
```{r}
simtree1 <- tree(Rank~.,data=data1)
summary(simtree1)
plot(simtree1)
text(simtree1, pretty=0)
```

KNN-Classification
```{r}
set.seed(450)
train<-createDataPartition(y=data$Rank,p=.7,list=F)
training<-data[train,]
testing<-data[-train,]
```

Preforming 10 fold cross validation repeated three times
```{r}
control<-trainControl(method="repeatedcv",number = 10,repeats=3)
knn<-train(Rank~.,data=training,method="knn",trControl=control,preProcess=c("center","scale"),tuneLength=20)
knn

plot(knn)
test<-predict(knn,newdata = testing)
mean((testing$Rank-test)^2)

```

KNN MSE & more
```{r}
all.MSE<-data.frame(MSE=c("793.363","379.7","39.64","1.71","374"),Model=c("knn classification","regression tree","Multiple linear regression","Random Forest","regression tree"))

ggplot(data=all.MSE,aes(x=Model,y=MSE)) +
  geom_bar(stat="identity")
```

PCA + K-Means
Cleaning Data for PCA:
```{r}
data_clean <- data[,-c(1,2)]
```

PCA
```{r}
set.seed(450)
#ALL PRINCIPAL COMPONENTS PRESENT
pComp <- prcomp(as.matrix(data_clean[,-c(2)]), scale = TRUE, center = TRUE)
biplot(pComp, main="PC BiPlot")
plot(pComp, main = "Screeplot A")
plot(pComp, type='l', main = "Screeplot B")
allcomp <- data.frame(pComp$x[,1:5])
plot(allcomp, pch=16, col=rgb(0,0,0,0.5), main="All PC's Projection")
#Kaiser recommends to keep 1 PC
#To keep 95% of the variance keep 2 PC's
#screeplot recommends to keep 2 PC's

#WITH TWO PRINCIPLAL COMPONENTS RETAINED
retainedcomp <- data.frame(pComp$x[,1:2])
plot(retainedcomp, pch=16, col=rgb(0,0,0,0.5), main="Retainded PC1 and PC2 Projection")
summary(pComp)
round(pComp$rotation[,1:5],2)

```

K-means Clustering
```{r}
set.seed(450)
#Determining Number of Clusters
withinSumSquares <- (nrow(data_clean)-1)*sum(apply(data_clean,2,var))
for (i in 2:535) withinSumSquares[i] <- sum(kmeans(data_clean,centers=i)$withinss)
plot(1:535, withinSumSquares, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares", main="Within groups sum of squares")
#We should choose k=5 as shown from the Within groups sum of squares plot.
myKmean <- kmeans((retainedcomp), 5)
myKmean
palette(alpha(brewer.pal(9,'Set1'), 0.5))
plot(allcomp, col=myKmean$cluster, pch=16, main="All PC's K-Means Cluster Projection")
plot(retainedcomp, col=myKmean$cluster, pch=16, main="Retained PC's K-Means Cluster Projection")
# Centers
myKmean$centers
# Cluster sizes
groups <- data.frame(myKmean$cluster)
table(groups)
# Cities by cluster
data[myKmean$cluster==1,]
data[myKmean$cluster==2,]
data[myKmean$cluster==3,]
data[myKmean$cluster==4,]
data[myKmean$cluster==5,]

```

Random Forest and Neural Network Regression
First draft Random Forest with no training/testing data seperation
```{r}
set.seed(450)
cityRate1 = randomForest(Cost.of.Living.Index ~ Rank + Rent.Index + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index, data = data, importance = TRUE, ntree = 500, mtry = 2)#limit randomise variable for forest
cityRate1
importance(cityRate1)
summary(cityRate1)
prediction1 = predict(cityRate1, data)
solution1 = data.frame(City = data$City, Cost.of.Living.Index = prediction1)
solution1
varImpPlot(cityRate1)
```

Random Forest Analysis with test and training to predict Cost of Living
```{r}
set.seed(450)
trainindex <- sample(1:nrow(data), 200)
train <- droplevels(data[trainindex, ])
test <- droplevels(data[-trainindex, ])
#cityRate2 = randomForest(Cost.of.Living.Index ~ Rank + Rent.Index + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index, data = train, ntree = 500, mtry = 2, importance = TRUE)#limit randomise variable for forest
cityRate2 = randomForest(Cost.of.Living.Index ~ Rank + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index, data = train, ntree = 100, mtry = 2, importance = TRUE, do.trace=T)#limit randomise variable for forest, shown that rent and local purchasing power were not very important at all compared to other variables
cityRate2
importance(cityRate2)
summary(cityRate2)
prediction2 = predict(cityRate2, test)
#prediction
solution2 = data.frame(City = test$City, Cost.of.Living.Index = prediction2)
solution2
mean(cityRate2$mse) #mse
varImpPlot(cityRate2)
```

Neural Net Analysis
```{r}
set.seed(450)
nums = unlist(lapply(data, is.numeric))  
dataNums = data[ , nums]
scar <- apply(dataNums, 2, function(v) (v-min(v))/(max(v)-min(v)))
ind <- sample(1:nrow(scar), 300)
train <- scar[ind, ]
test <- scar[-ind, ]
nn = neuralnet(Cost.of.Living.Index ~ Rank + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index, data=scar, hidden=c(5,3), linear.output = TRUE)
nn$result.matrix
plot(nn)
summary(nn)
```

Using test data for accuracy rate (NN)
```{r}
nn2 = neuralnet(Cost.of.Living.Index ~ Rank + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index, data=train, hidden=c(5,3), linear.output = TRUE)
nn.results <- compute(nn2, test)
result <- nn.results$net.result
#result
sum((compute(nn2, test[,-2])$net.result-test[,2])^2) #mse
```

K-Fold - Neural Net
```{r}
# 10 fold cross validation
k <- 10
# Results from cv
outs <- NULL
# Train test split proportions
proportion <- 0.95 # Set to 0.995 for LOOCV

for(i in 1:k)
{
    index <- sample(1:nrow(scar), round(proportion*nrow(scar)))
    train_cv <- scar[index, ]
    test_cv <- scar[-index, ]
    nn_cv <- neuralnet(Cost.of.Living.Index ~ Rank + Rent.Index + Cost.of.Living.Plus.Rent.Index + Groceries.Index + Restaurant.Price.Index + Local.Purchasing.Power.Index,
                        data = train,
                        hidden = c(13, 10, 3),
                        act.fct = "logistic",
                        linear.output = FALSE)
    
    # Compute predictions
    pr.nn <- compute(nn_cv, test)
    # Extract results
    pr.nn_ <- pr.nn$net.result
    # Accuracy (test set)
    original_values <- max.col(test)
    pr.nn_2 <- max.col(pr.nn_)
    outs[i] <- mean(pr.nn_2 == original_values)
}
outs
mean(outs)
``` 
